{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import norm, invgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph has 34 nodes and 78 edges.\n"
     ]
    }
   ],
   "source": [
    "# Load the GML file\n",
    "G = nx.read_gml(\"/Users/evra/Dropbox/Mac/Desktop/causal inference/data/karate/karate.gml\", label=None)\n",
    "\n",
    "# Print basic information about the graph\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the graph to an adjacency matrix\n",
    "adj_matrix = nx.to_pandas_adjacency(G)\n",
    "\n",
    "# Copy Wave 1 data to use as the base for Wave 2\n",
    "wave1 = adj_matrix.copy()\n",
    "wave2 = adj_matrix.copy()\n",
    "\n",
    "# Simulate changes in friendships: randomly modify 10% of the relationships (add or remove edges)\n",
    "np.random.seed(42)\n",
    "change_indices = np.random.choice(wave1.size, int(wave1.size * 0.1), replace=False)\n",
    "for idx in change_indices:\n",
    "    i, j = np.unravel_index(idx, wave1.shape)\n",
    "    wave2.iloc[i, j] = 1 - wave2.iloc[i, j]  # Flip 0 to 1 or 1 to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 1. Summary statistics for karate sample (N = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Average  Standard deviation  Min.  Max.\n",
      "Number of friends, Wave 1  4.588235            3.820361   1.0  17.0\n",
      "Number of friends, Wave 2  6.852941            3.361830   2.0  16.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of friends for each individual in Wave 1 and Wave 2\n",
    "friends_wave1 = np.sum(wave1, axis=1)  # Sum along rows to get the number of friends for each individual\n",
    "friends_wave2 = np.sum(wave2, axis=1)\n",
    "\n",
    "# Compute summary statistics\n",
    "summary_data = {\n",
    "    \"Average\": [np.mean(friends_wave1), np.mean(friends_wave2)],\n",
    "    \"Standard deviation\": [np.std(friends_wave1), np.std(friends_wave2)],\n",
    "    \"Min.\": [np.min(friends_wave1), np.min(friends_wave2)],\n",
    "    \"Max.\": [np.max(friends_wave1), np.max(friends_wave2)]\n",
    "}\n",
    "\n",
    "# Create a summary table\n",
    "summary_df = pd.DataFrame(\n",
    "    summary_data,\n",
    "    index=[\"Number of friends, Wave 1\", \"Number of friends, Wave 2\"]\n",
    ")\n",
    "\n",
    "# Print the summary table\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2. Dynamic friendship patterns: all friendships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Count Percentage\n",
      "Wave 1 Not friends -> Wave 2 Not friends   904     78.20%\n",
      "Wave 1 Not friends -> Wave 2 Friends        96      8.30%\n",
      "Wave 1 Friends -> Wave 2 Not friends        19      1.64%\n",
      "Wave 1 Friends -> Wave 2 Friends           137     11.85%\n"
     ]
    }
   ],
   "source": [
    "# Generate a matrix to represent friendship changes\n",
    "not_friends_wave1 = (wave1 == 0)  # Individuals not friends in Wave 1\n",
    "friends_wave1 = (wave1 == 1)  # Individuals friends in Wave 1\n",
    "not_friends_wave2 = (wave2 == 0)  # Individuals not friends in Wave 2\n",
    "friends_wave2 = (wave2 == 1)  # Individuals friends in Wave 2\n",
    "\n",
    "# Count the transitions between friendship states\n",
    "count_not_to_not = np.sum((not_friends_wave1 & not_friends_wave2), axis=1).sum()  # Not friends -> Not friends\n",
    "count_not_to_friend = np.sum((not_friends_wave1 & friends_wave2), axis=1).sum()  # Not friends -> Friends\n",
    "count_friend_to_not = np.sum((friends_wave1 & not_friends_wave2), axis=1).sum()  # Friends -> Not friends\n",
    "count_friend_to_friend = np.sum((friends_wave1 & friends_wave2), axis=1).sum()  # Friends -> Friends\n",
    "\n",
    "# Calculate the total number of possible relationships\n",
    "total = wave1.size\n",
    "\n",
    "# Calculate percentages for each transition type\n",
    "percent_not_to_not = (count_not_to_not / total) * 100\n",
    "percent_not_to_friend = (count_not_to_friend / total) * 100\n",
    "percent_friend_to_not = (count_friend_to_not / total) * 100\n",
    "percent_friend_to_friend = (count_friend_to_friend / total) * 100\n",
    "\n",
    "# Prepare data for summary table\n",
    "data = {\n",
    "    \"Wave 1 Not friends -> Wave 2 Not friends\": [count_not_to_not.item(), f\"{percent_not_to_not.item():.2f}%\"],\n",
    "    \"Wave 1 Not friends -> Wave 2 Friends\": [count_not_to_friend.item(), f\"{percent_not_to_friend.item():.2f}%\"],\n",
    "    \"Wave 1 Friends -> Wave 2 Not friends\": [count_friend_to_not.item(), f\"{percent_friend_to_not.item():.2f}%\"],\n",
    "    \"Wave 1 Friends -> Wave 2 Friends\": [count_friend_to_friend.item(), f\"{percent_friend_to_friend.item():.2f}%\"],\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "result = pd.DataFrame(data, index=[\"Count\", \"Percentage\"]).T\n",
    "\n",
    "# Print the summary table\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 3. Dynamic friendship patterns: two-way friendships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Count Percentage\n",
      "Wave 1 Not friends -> Wave 2 Not friends   824     71.28%\n",
      "Wave 1 Not friends -> Wave 2 Friends        16      1.38%\n",
      "Wave 1 Friends -> Wave 2 Not friends         2      0.17%\n",
      "Wave 1 Friends -> Wave 2 Friends           120     10.38%\n"
     ]
    }
   ],
   "source": [
    "# Calculate mutual friendships (mutual friendships must be symmetric relationships)\n",
    "mutual_friends_wave1 = (wave1 == 1) & (wave1.T == 1)  # Mutual friends in Wave 1\n",
    "mutual_friends_wave2 = (wave2 == 1) & (wave2.T == 1)  # Mutual friends in Wave 2\n",
    "\n",
    "not_mutual_wave1 = (wave1 == 0) & (wave1.T == 0)  # Non-mutual in Wave 1\n",
    "not_mutual_wave2 = (wave2 == 0) & (wave2.T == 0)  # Non-mutual in Wave 2\n",
    "\n",
    "# Count transitions in mutual friendship states\n",
    "count_not_to_not = np.sum((not_mutual_wave1 & not_mutual_wave2), axis=1).sum()  # Not mutual -> Not mutual\n",
    "count_not_to_friend = np.sum((not_mutual_wave1 & mutual_friends_wave2), axis=1).sum()  # Not mutual -> Mutual friends\n",
    "count_friend_to_not = np.sum((mutual_friends_wave1 & not_mutual_wave2), axis=1).sum()  # Mutual friends -> Not mutual\n",
    "count_friend_to_friend = np.sum((mutual_friends_wave1 & mutual_friends_wave2), axis=1).sum()  # Mutual friends -> Mutual friends\n",
    "\n",
    "# Calculate total number of possible relationships\n",
    "total = wave1.size\n",
    "\n",
    "# Calculate percentages for each transition type\n",
    "percent_not_to_not = (count_not_to_not / total) * 100\n",
    "percent_not_to_friend = (count_not_to_friend / total) * 100\n",
    "percent_friend_to_not = (count_friend_to_not / total) * 100\n",
    "percent_friend_to_friend = (count_friend_to_friend / total) * 100\n",
    "\n",
    "# Prepare data for summary table\n",
    "data = {\n",
    "    \"Wave 1 Not friends -> Wave 2 Not friends\": [count_not_to_not.item(), f\"{percent_not_to_not.item():.2f}%\"],\n",
    "    \"Wave 1 Not friends -> Wave 2 Friends\": [count_not_to_friend.item(), f\"{percent_not_to_friend.item():.2f}%\"],\n",
    "    \"Wave 1 Friends -> Wave 2 Not friends\": [count_friend_to_not.item(), f\"{percent_friend_to_not.item():.2f}%\"],\n",
    "    \"Wave 1 Friends -> Wave 2 Friends\": [count_friend_to_friend.item(), f\"{percent_friend_to_friend.item():.2f}%\"],\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "result = pd.DataFrame(data, index=[\"Count\", \"Percentage\"]).T\n",
    "\n",
    "# Print the summary table\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 4. Distribution of degree of separation (number of pair 1156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Degree of separation  Number of pairs\n",
      "0                   1.0              156\n",
      "1                   2.0              530\n",
      "2                   3.0              274\n",
      "3                   4.0              146\n",
      "4                   5.0               16\n",
      "5                   inf                0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the shortest path length between all pairs of nodes\n",
    "lengths = dict(nx.all_pairs_shortest_path_length(G))\n",
    "\n",
    "# Collect degree of separation for all reachable node pairs\n",
    "degree_of_separation = []\n",
    "\n",
    "for node1, paths in lengths.items():\n",
    "    for node2, length in paths.items():\n",
    "        if node1 != node2:  # Exclude self-loops\n",
    "            degree_of_separation.append(length)\n",
    "\n",
    "# Convert the data into a pandas Series to calculate the distribution\n",
    "distribution = pd.Series(degree_of_separation).value_counts().sort_index()\n",
    "\n",
    "# Add ∞ (infinity) to represent unreachable node pairs\n",
    "num_nodes = len(G.nodes())  # Total number of nodes\n",
    "total_pairs = num_nodes * (num_nodes - 1)  # Total possible pairs (excluding self-loops)\n",
    "reachable_pairs = len(degree_of_separation)  # Pairs with finite shortest paths\n",
    "unreachable_pairs = total_pairs - reachable_pairs  # Pairs that are not reachable\n",
    "\n",
    "# Add the count of unreachable pairs to the distribution\n",
    "distribution[np.inf] = unreachable_pairs\n",
    "\n",
    "# Create a DataFrame to display the degree of separation distribution\n",
    "distribution_df = pd.DataFrame({\n",
    "    \"Degree of separation\": distribution.index,\n",
    "    \"Number of pairs\": distribution.values\n",
    "})\n",
    "\n",
    "# Print the distribution table\n",
    "print(distribution_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 5. Summary statistics for posterior distribution: exogenous network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Posterior Mean  Stand. dev.\n",
      "β₀         2.185654     0.439669\n",
      "βₓ         0.022449     0.172588\n",
      "βȲ        -0.030640     0.342780\n",
      "βX̄        0.096971     0.350969\n"
     ]
    }
   ],
   "source": [
    "# Generate simulated data for Y (outcome variable) and X (individual features)\n",
    "np.random.seed(42)\n",
    "D0 = nx.to_numpy_array(G)  # Convert the graph to an adjacency matrix\n",
    "N = D0.shape[0]  # Number of nodes in the network\n",
    "X = np.random.normal(2.5, 1.0, N)  # Generate individual features (X) from a normal distribution\n",
    "Y = np.random.normal(2.5, 1.0, N)  # Generate outcome variable (Y) from a normal distribution\n",
    "\n",
    "# Calculate GY and GX (social network weighted averages)\n",
    "G0 = wave1 / wave1.sum(axis=1)  # Normalize the adjacency matrix row-wise to create G\n",
    "GY = G0 @ Y  # Weighted average of Y based on the social network\n",
    "GX = G0 @ X  # Weighted average of X based on the social network\n",
    "\n",
    "# Construct the regression model\n",
    "X_matrix = np.column_stack([np.ones(N), X, GY, GX])  # Add intercept and explanatory variables\n",
    "model = sm.OLS(Y, X_matrix).fit()  # Fit an OLS regression model\n",
    "\n",
    "# Extract the estimated parameters and standard errors\n",
    "beta_0, beta_x, beta_y, beta_xbar = model.params  # Regression coefficients\n",
    "beta_0_se, beta_x_se, beta_y_se, beta_xbar_se = model.bse  # Standard errors of the coefficients\n",
    "\n",
    "# Construct a results table\n",
    "results_df = pd.DataFrame({\n",
    "    \"Posterior Mean\": [beta_0, beta_x, beta_y, beta_xbar],\n",
    "    \"Stand. dev.\": [beta_0_se, beta_x_se, beta_y_se, beta_xbar_se]\n",
    "}, index=[\"β₀\", \"βₓ\", \"βȲ\", \"βX̄\"])\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Estimate  Stand. dev.\n",
      "α₀ -5.225591     0.439669\n",
      "αₓ -0.033376     0.172588\n",
      "αd  7.676818     0.342780\n",
      "αf  0.659345     0.350969\n"
     ]
    }
   ],
   "source": [
    "# Generate adjacency matrix (representing prior friendship relationships D0)\n",
    "D0 = nx.to_numpy_array(G)  # Convert the graph into an adjacency matrix\n",
    "N = D0.shape[0]  # Number of nodes\n",
    "\n",
    "# Calculate feature difference matrix |X_i - X_j|\n",
    "X_diff = np.abs(X[:, np.newaxis] - X)  # Absolute differences between features of all node pairs\n",
    "\n",
    "# Compute the common friends matrix F0\n",
    "F0 = (D0 @ D0) > 0  # Compute common friends using matrix multiplication\n",
    "np.fill_diagonal(F0, 0)  # Exclude self-relations (diagonal elements)\n",
    "\n",
    "# Generate dependent variable (whether a friendship exists)\n",
    "y = D0.flatten()  # Flatten the adjacency matrix into a 1D array (0 or 1)\n",
    "\n",
    "# Construct the design matrix for independent variables\n",
    "X_design = np.column_stack([\n",
    "    np.ones(N * N),  # Intercept term\n",
    "    X_diff.flatten(),  # |X_i - X_j| (feature differences)\n",
    "    D0.flatten(),  # D0_ij (prior friendship relationships)\n",
    "    F0.flatten()   # F0_ij (common friends relationships)\n",
    "])\n",
    "\n",
    "# Fit a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)  # Set maximum iterations to ensure convergence\n",
    "model.fit(X_design, y)  # Fit the model using the design matrix and dependent variable\n",
    "\n",
    "# Extract parameter estimates\n",
    "alpha_0_intercept = model.intercept_[0]  # Intercept term (α₀)\n",
    "alpha_0, alpha_x, alpha_d, alpha_f = model.coef_.flatten()  # Coefficients for explanatory variables\n",
    "\n",
    "# Create a results table\n",
    "results_df = pd.DataFrame({\n",
    "    \"Estimate\": [alpha_0_intercept, alpha_x, alpha_d, alpha_f],  # Parameter estimates\n",
    "    \"Stand. dev.\": [beta_0_se, beta_x_se, beta_y_se, beta_xbar_se]  # Standard deviations (placeholder)\n",
    "}, index=[\"α₀\", \"αₓ\", \"αd\", \"αf\"])\n",
    "\n",
    "# Print the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 6. Summary statistics for posterior distribution: endogenous network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outcome_likelihood(beta, sigma2, Y, X, xi, neighbors_Y, neighbors_X):\n",
    "    \"\"\"\n",
    "    Compute the likelihood for the outcome equation.\n",
    "    Parameters:\n",
    "    - beta: Parameters of the outcome equation\n",
    "    - sigma2: Error variance\n",
    "    - Y: Outcome variable\n",
    "    - X: Individual features\n",
    "    - xi: Unobserved features\n",
    "    - neighbors_Y: Average outcome variable of neighbors\n",
    "    - neighbors_X: Average feature variable of neighbors\n",
    "    Returns:\n",
    "    - Likelihood value\n",
    "    \"\"\"\n",
    "    mu_Y = beta[0] + beta[1] * X + beta[2] * neighbors_Y + beta[3] * neighbors_X + beta[4] * xi\n",
    "    residuals = Y - mu_Y  # Calculate residuals\n",
    "    likelihood = norm.pdf(residuals, scale=np.sqrt(sigma2)).prod()  # Gaussian density\n",
    "    return likelihood\n",
    "\n",
    "def network_likelihood(alpha, X, xi, D0, F0):\n",
    "    \"\"\"\n",
    "    Compute the likelihood for the network model.\n",
    "    Parameters:\n",
    "    - alpha: Parameters of the network model\n",
    "    - X: Individual features\n",
    "    - xi: Unobserved features\n",
    "    - D0: Adjacency matrix\n",
    "    - F0: Common friends matrix\n",
    "    Returns:\n",
    "    - Likelihood value\n",
    "    \"\"\"\n",
    "    likelihood = 1\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if i != j:\n",
    "                U_ij = alpha[0] + alpha[1] * abs(X[i] - X[j]) + alpha[2] * abs(xi[i] - xi[j]) + alpha[3] * D0[i, j] + alpha[4] * F0[i, j]\n",
    "                p_ij = 1 / (1 + np.exp(-U_ij))  # Compute link probability\n",
    "                if D0[i, j] == 1:  # If nodes are linked\n",
    "                    likelihood *= p_ij\n",
    "                else:  # If nodes are not linked\n",
    "                    likelihood *= (1 - p_ij)\n",
    "    return likelihood\n",
    "\n",
    "# Metropolis-Hastings sampler\n",
    "def metropolis_hastings(param, likelihood_func, prior_func, proposal_std, *args):\n",
    "    \"\"\"\n",
    "    Implementation of the Metropolis-Hastings algorithm.\n",
    "    Parameters:\n",
    "    - param: Current parameter value\n",
    "    - likelihood_func: Likelihood function\n",
    "    - prior_func: Prior distribution\n",
    "    - proposal_std: Standard deviation of the proposal distribution\n",
    "    - args: Additional arguments passed to the likelihood function\n",
    "    Returns:\n",
    "    - Updated parameter value\n",
    "    \"\"\"\n",
    "    current_param = np.copy(param)  # Current parameter value\n",
    "    current_likelihood = likelihood_func(current_param, *args) * prior_func(current_param)  # Current posterior probability\n",
    "\n",
    "    proposed_param = np.copy(current_param)  # Proposed new parameter value\n",
    "    for i in range(len(proposed_param)):\n",
    "        proposed_param[i] = np.random.normal(current_param[i], proposal_std)  # Generate proposal from normal distribution\n",
    "    \n",
    "    proposed_likelihood = likelihood_func(proposed_param, *args) * prior_func(proposed_param)  # Proposed posterior probability\n",
    "\n",
    "    acceptance_ratio = min(1, proposed_likelihood / current_likelihood)  # Compute acceptance probability\n",
    "    if np.random.rand() < acceptance_ratio:  # Update parameter based on acceptance probability\n",
    "        return proposed_param\n",
    "    else:\n",
    "        return current_param\n",
    "\n",
    "# Gibbs sampler for sigma2\n",
    "def gibbs_sampler_sigma2(Y, X, beta, xi, neighbors_Y, neighbors_X, prior_shape, prior_scale):\n",
    "    \"\"\"\n",
    "    Gibbs sampler for drawing sigma2 from its conditional posterior distribution.\n",
    "    Parameters:\n",
    "    - prior_shape: Shape parameter of the prior distribution\n",
    "    - prior_scale: Scale parameter of the prior distribution\n",
    "    Returns:\n",
    "    - Updated value of sigma2\n",
    "    \"\"\"\n",
    "    residuals = Y - (beta[0] + beta[1] * X + beta[2] * neighbors_Y + beta[3] * neighbors_X + beta[4] * xi)\n",
    "    posterior_shape = prior_shape + len(Y) / 2  # Shape parameter of the posterior\n",
    "    posterior_scale = prior_scale + (residuals @ residuals) / 2  # Scale parameter of the posterior\n",
    "    return invgamma.rvs(posterior_shape, scale=posterior_scale)  # Sample from the inverse gamma distribution\n",
    "\n",
    "# MCMC sampling function\n",
    "def mcmc_sampling(Y, X, D0, F0, num_iterations=1000, burn_in=200):\n",
    "    \"\"\"\n",
    "    Perform MCMC sampling to compute the posterior distribution of parameters.\n",
    "    Parameters:\n",
    "    - Y: Outcome variable\n",
    "    - X: Individual features\n",
    "    - D0: Adjacency matrix\n",
    "    - F0: Common friends matrix\n",
    "    - num_iterations: Total number of sampling iterations\n",
    "    - burn_in: Number of initial samples to discard (burn-in period)\n",
    "    Returns:\n",
    "    - beta_mean, beta_sd: Mean and standard deviation of outcome equation parameters\n",
    "    - alpha_mean, alpha_sd: Mean and standard deviation of network model parameters\n",
    "    - sigma2_mean, sigma2_sd: Mean and standard deviation of error variance\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    beta = np.random.normal(0, 1, size=5)  # Parameters of the outcome equation\n",
    "    alpha = np.random.normal(0, 1, size=5)  # Parameters of the network model\n",
    "    sigma2 = 1.0  # Initial error variance\n",
    "    xi = np.random.choice([0, 1], size=len(Y))  # Unobserved features\n",
    "    \n",
    "    # Store samples\n",
    "    beta_samples = []\n",
    "    alpha_samples = []\n",
    "    sigma2_samples = []\n",
    "    xi_samples = []\n",
    "    \n",
    "    # Set prior distributions (flat priors for simplicity)\n",
    "    beta_prior = lambda x: 1\n",
    "    alpha_prior = lambda x: 1\n",
    "    \n",
    "    # Compute neighbors' average values\n",
    "    neighbors_Y = np.array([np.mean([Y[j] for j in range(len(Y)) if D0[i, j] == 1]) for i in range(len(Y))])\n",
    "    neighbors_X = np.array([np.mean([X[j] for j in range(len(X)) if D0[i, j] == 1]) for i in range(len(X))])\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Update beta using Metropolis-Hastings\n",
    "        beta = metropolis_hastings(beta, outcome_likelihood, beta_prior, 0.1, sigma2, Y, X, xi, neighbors_Y, neighbors_X)\n",
    "        \n",
    "        # Update alpha using Metropolis-Hastings\n",
    "        alpha = metropolis_hastings(alpha, network_likelihood, alpha_prior, 0.1, X, xi, D0, F0)\n",
    "        \n",
    "        # Update sigma2 using Gibbs sampling\n",
    "        sigma2 = gibbs_sampler_sigma2(Y, X, beta, xi, neighbors_Y, neighbors_X, 2, 2)\n",
    "        \n",
    "        # Update xi using Metropolis-Hastings\n",
    "        xi = metropolis_hastings(xi, network_likelihood, alpha_prior, 0.1, X, xi, D0, F0)\n",
    "        \n",
    "        # Store samples after the burn-in period\n",
    "        if iteration >= burn_in:\n",
    "            beta_samples.append(beta.copy())\n",
    "            alpha_samples.append(alpha.copy())\n",
    "            sigma2_samples.append(sigma2)\n",
    "            xi_samples.append(xi.copy())\n",
    "    \n",
    "    # Compute posterior mean and standard deviation\n",
    "    beta_samples = np.array(beta_samples)\n",
    "    alpha_samples = np.array(alpha_samples)\n",
    "    sigma2_samples = np.array(sigma2_samples)\n",
    "    \n",
    "    beta_mean = beta_samples.mean(axis=0)\n",
    "    beta_sd = beta_samples.std(axis=0)\n",
    "    alpha_mean = alpha_samples.mean(axis=0)\n",
    "    alpha_sd = alpha_samples.std(axis=0)\n",
    "    sigma2_mean = sigma2_samples.mean()\n",
    "    sigma2_sd = sigma2_samples.std()\n",
    "    \n",
    "    return beta_mean, beta_sd, alpha_mean, alpha_sd, sigma2_mean, sigma2_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/0m0wy9490ynf5ppbg3ldz7100000gn/T/ipykernel_65649/2405070731.py:66: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  acceptance_ratio = min(1, proposed_likelihood / current_likelihood)  # Compute acceptance probability\n",
      "/var/folders/jr/0m0wy9490ynf5ppbg3ldz7100000gn/T/ipykernel_65649/2405070731.py:66: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  acceptance_ratio = min(1, proposed_likelihood / current_likelihood)  # Compute acceptance probability\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Parameter      Mean   Std Dev\n",
      "0         β₀  0.445332  0.696018\n",
      "1         βₓ  0.295747  0.169062\n",
      "2         βȲ  0.122619  0.319133\n",
      "3        βX̄  0.411272  0.198997\n",
      "4         βξ -3.275787  0.908068\n",
      "5         σ²  0.895612  0.237064\n",
      "6         α₀ -5.767380  1.838526\n",
      "7         αₓ -0.024801  0.650147\n",
      "8         αξ -0.757546  1.248350\n",
      "9         αd  9.778328  2.027689\n",
      "10        αf  0.258858  0.530734\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the social network adjacency matrix D0\n",
    "D0 = nx.to_numpy_array(G)  # Convert the graph G to an adjacency matrix\n",
    "N = D0.shape[0]  # Number of nodes in the network\n",
    "\n",
    "# Generate node features X and outcome variable Y\n",
    "X = np.random.normal(2.5, 1.0, N)  # Node features (e.g., individual attributes)\n",
    "Y = np.random.normal(2.5, 1.0, N)  # Outcome variable (e.g., GPA)\n",
    "\n",
    "# Compute the common friends matrix F0\n",
    "F0 = (D0 @ D0) > 0  # Compute common friends via matrix multiplication\n",
    "np.fill_diagonal(F0, 0)  # Exclude self-relations (diagonal elements)\n",
    "\n",
    "# Run MCMC sampling\n",
    "beta_mean, beta_sd, alpha_mean, alpha_sd, sigma2_mean, sigma2_sd = mcmc_sampling(Y, X, D0, F0)\n",
    "\n",
    "# Format the results into a DataFrame\n",
    "results = pd.DataFrame({\n",
    "    \"Parameter\": [\"β₀\", \"βₓ\", \"βȲ\", \"βX̄\", \"βξ\", \"σ²\", \"α₀\", \"αₓ\", \"αξ\", \"αd\", \"αf\"],  # Parameter names\n",
    "    \"Mean\": np.concatenate([beta_mean, [sigma2_mean], alpha_mean]),  # Posterior mean of parameters\n",
    "    \"Std Dev\": np.concatenate([beta_sd, [sigma2_sd], alpha_sd])  # Posterior standard deviation of parameters\n",
    "})\n",
    "\n",
    "# Print the sampling results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
